{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13207703-5109-4860-8495-2ef4e8d2b3e1",
   "metadata": {},
   "source": [
    "# SENTENCE CLASSIFICATION\n",
    "\n",
    "This notebook shows how to fine-tune a pretrainde model from [HugginFace](https://huggingface.co/) using the [Trainer module](https://huggingface.co/docs/transformers/main_classes/trainer).\n",
    "\n",
    "## 1. Loading packages and dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ff6660-1105-4ee3-8701-aeba56260e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f24a924-20fd-4e5f-908c-28eac8a39a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_PATH = <DEFINE_THE_REPOSITORY_PATH>\n",
    "os.chdir(PROJECT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b641bb-a623-4db4-ba56-c948e2354df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.config import CLASSES\n",
    "from src.sentence_clf.data_loader import ClassificationDataset\n",
    "from src.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb29ed02-7b59-411d-b667-bae730b0144a",
   "metadata": {},
   "source": [
    "## 2. Loading the Tokenizer and the pretrained model\n",
    "\n",
    "We define here the pretrained model that we will to build a sentence classifier. And then, we will use a sample data file that is available in the repository `data/sample_data.csv` . This file contains sentences in French and it includes **4 classes**.\n",
    "\n",
    "Since the sentences are in French, we have chosen a well-known pretrained model: **camembert-base**.\n",
    "\n",
    "**NOTE:** Yous should use the same model name to load the Tokenizer, since each pretrained model has its own tokenizer. Using a different Tokenizer could cause some problems during the training or the evaluation phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325bb119-b9e6-43d3-b7f6-9252fc1b5f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"camembert-base\"\n",
    "nb_labels = 4\n",
    "\n",
    "# Set the model and the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "clf_model = AutoModelForSequenceClassification.from_pretrained(model, num_labels=nb_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630033b5-2302-43d4-b25a-e17252627c61",
   "metadata": {},
   "source": [
    "## 3. Preparing the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ac7a88-deae-44e0-9580-0ed69715e568",
   "metadata": {},
   "source": [
    "We have defined the **sentence classes** in the `src/config.py` file. To fine-tune the pre-trained model to perform a multiclass classification task, we should transform our **target variable** using a **one-hot encoder** before to train the model, since we have many output in the model as the number of classes that we want to detect.\n",
    "\n",
    "For example, if we want to classify the sentences in 4 classes, then the target variable should have a 4 binary representation one per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25867fd8-a0c3-4a53-b3c8-06eabfb03b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = get_on_hot_encoder(data=list(CLASSES.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ce6be2-b22f-4768-a925-b990600fde5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training and validation sets\n",
    "train_df = pd.read_csv(\"data/sample_data.csv\", sep='|')\n",
    "\n",
    "# Map the classes to numeric values\n",
    "train_df['new_label'] = train_df['labels'].map(CLASSES)\n",
    "\n",
    "# Use the one-hot encoder to transform our target variable\n",
    "train_df['encoded_label'] = train_df[\"new_label\"].apply(\n",
    "    lambda x: encoder.transform([[x]]).toarray().tolist()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812bce05-6771-4e9a-9504-3446540e5fac",
   "metadata": {},
   "source": [
    "We split the training set into **training** and **validation** sets using only the **text** and the **encoded label** columns. Then, we should prepare these data sets to feed them to the Trainer of HugginFace. For this, we should tokenize the text using the **Tokenizer** and then we use the Dataset Loader that allows to load the data set by batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132920d9-3164-471f-9952-139b8614fff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation sets\n",
    "texts = train_df[\"text\"].tolist()\n",
    "labels = train_df[\"encoded_label\"].tolist()\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    texts, labels, test_size=.1, random_state=17, stratify=labels\n",
    ")\n",
    "\n",
    "# Prepare train and val sets for the training\n",
    "train_encodings = tokenizer(train_texts, truncation=True, max_length=300,\n",
    "                            padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, max_length=300,\n",
    "                          padding=True)\n",
    "\n",
    "train_dataset = ClassificationDataset(train_encodings, train_labels)\n",
    "val_dataset = ClassificationDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b81d9f-c779-4d5d-bab4-ba252ff59ad4",
   "metadata": {},
   "source": [
    "## 4. Configure the Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6ecc47-ae9e-42c1-9426-3c758b12989e",
   "metadata": {},
   "source": [
    "To fine-tune the pretrained model, we use the **Trainer** module from HugginFace. For this, we should first configure the training arguments to use during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483bd1ae-af36-46fc-b045-d5a7e25b4cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config the Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output_model\",     # output directory\n",
    "    num_train_epochs=5,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=20,                # number of steps before to store training metrics\n",
    "    evaluation_strategy=\"steps\",     # strategy to compute the training metrics\n",
    "    save_strategy=\"steps\",           # should be the same as evaluation_strategy\n",
    "    load_best_model_at_end=True,     # load the best model at the end of the training\n",
    "    report_to=\"none\",                # useful if used with mlflow for training reporting\n",
    "    run_name=\"none\",                 # name of the run to report to mlflow\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322f863f-e365-47d7-8988-a308bf906aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the Trainer\n",
    "trainer = Trainer(\n",
    "    model=clf_model,                  # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,               # training arguments, defined above\n",
    "    train_dataset=train_dataset,      # training dataset\n",
    "    eval_dataset=val_dataset,         # evaluation dataset\n",
    "    compute_metrics=compute_metrics,  # function to compute the metrics during the training\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3485f0d5-00dc-4c99-a02e-3f43aca958d9",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd35164-3128-4db4-b504-a4bf25815af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Trainer\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0af942-424e-4d4c-82d8-64d00634d8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the BEST MODEL\n",
    "trainer.save_model(output_dir=\"./output_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2cac1b-53b4-4e19-aee2-2429bbb69f81",
   "metadata": {},
   "source": [
    "## 6. Evaluation - Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f458785c-5e56-4e74-930d-7021198ac274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing metrics on training and validation sets\n",
    "train_pred = trainer.predict(test_dataset=train_dataset, metric_key_prefix=\"train\")\n",
    "val_pred = trainer.predict(test_dataset=val_dataset, metric_key_prefix=\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2928b3-5679-4cf3-b469-f45e58ed035a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing metrics\n",
    "print(train_pred.metrics)\n",
    "print(val_pred.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9eaeabc-6c2c-460a-aabc-460fa5e9c387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loss evolution\n",
    "history = trainer.state.log_history\n",
    "plot_history_loss(\n",
    "    history=history,\n",
    "    output_file=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cb3f9b-b3e0-429e-94c3-165d778e41a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the VALIDATION SET\n",
    "inverse_rof_classes = {v: k for k, v in CLASSES.items()}\n",
    "\n",
    "val_df = pd.DataFrame(columns=[\"text\", \"true_label\", \"prediction_label\", \"prediction_score\"])\n",
    "val_df['text'] = val_texts\n",
    "val_df['true_label_nb'] = encoder.inverse_transform(val_labels)\n",
    "val_df['true_label'] = val_df['true_label_nb'].map(inverse_rof_classes)\n",
    "idx = np.argmax(val_pred.predictions, axis=-1)\n",
    "val_df['prediction_label_nb'] = idx\n",
    "val_df['prediction_label'] = val_df['prediction_label_nb'].map(inverse_rof_classes)\n",
    "scores = torch.nn.functional.softmax(torch.tensor(val_pred.predictions), dim=1).tolist()\n",
    "val_df['prediction_score'] = [score[index] for score, index in zip(scores, idx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53984deb-6fee-4502-8eb6-f49d94f67fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(\n",
    "    y_true=val_df[\"true_label_nb\"].tolist(),\n",
    "    y_pred=val_df[\"prediction_label_nb\"].tolist(),\n",
    "    labels=list(CLASSES.keys()),\n",
    "    output_file=None,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
